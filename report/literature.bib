@inproceedings{gemulla11,
 author = {Gemulla, Rainer and Nijkamp, Erik and Haas, Peter and Sismanis, Yannis},
 year = {2011},
 month = {08},
 pages = {69-77},
 title = {Large-scale matrix factorization with distributed stochastic gradient descent},
 journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 doi = {10.1145/2020408.2020426},
 url = {https://doi.org/10.1145/2020408.2020426}
}

@inproceedings{zhang17,
 author = {Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent},
 url = {https://proceedings.neurips.cc/paper/2017/file/f75526659f31040afeb61cb7133e4e6d-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{yu21,
 author = {Yu, Enda and Dong, Dezun and Xu, Yemao and Ouyang, Shuo and Liao, Xiangke},
 title = {CD-SGD: Distributed Stochastic Gradient Descent with Compression and Delay Compensation},
 year = {2021},
 isbn = {9781450390682},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/3472456.3472508},
 doi = {10.1145/3472456.3472508},
 abstract = { Communication overhead is the key challenge for distributed training. Gradient compression is a widely used approach to reduce communication traffic. When combining with a parallel communication mechanism method like pipeline, gradient compression technique can greatly alleviate the impact of communication overhead. However, there exist two problems of gradient compression technique to be solved. Firstly, gradient compression brings in extra computation cost, which will delay the next training iteration. Secondly, gradient compression usually leads to a decrease in convergence accuracy. In this paper, we combine parallel mechanism with gradient quantization and delayed full-gradient compensation, and propose a new distributed optimization method named CD-SGD, which can hide the overhead of gradient compression, overlap part of the communication and obtain high convergence accuracy. The local update operation in CD-SGD allows the next iteration to be launched quickly without waiting for the completion of gradient compression and the current communication process. Besides, the accuracy loss caused by gradient compression is solved by k-step correction method introduced in CD-SGD. We prove that CD-SGD has convergence guarantee and it achieves at least convergence rate. We conduct extensive experiments on MXNet to verify the convergence properties and scaling performance of CD-SGD. Experimental results on a 16-GPU cluster show that convergence accuracy of CD-SGD is close to or even slightly better than that of S-SGD, and its end-to-end time is 30 less than 2-bit gradient compression under a 56Gbps bandwidth environment. },
 booktitle = {50th International Conference on Parallel Processing},
 articleno = {79},
 numpages = {10},
 keywords = {Gradient Compression, Distributed Communication Optimization, Communication Mechanism Optimization},
 location = {Lemont, IL, USA},
 series = {ICPP 2021}
}

@article{ahn15,
 author    = {Sungjin Ahn and
   Anoop Korattikara Balan and
   Nathan Liu and
   Suju Rajan and
   Max Welling},
 title     = {Large-Scale Distributed Bayesian Matrix Factorization using Stochastic Gradient {MCMC}},
 journal   = {CoRR},
 volume    = {abs/1503.01596},
 year      = {2015},
 url       = {http://arxiv.org/abs/1503.01596},
 eprinttype = {arXiv},
 eprint    = {1503.01596},
 timestamp = {Mon, 13 Aug 2018 16:47:53 +0200},
 biburl    = {https://dblp.org/rec/journals/corr/AhnBLRW15.bib},
 bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{swenson20,
 doi = {10.48550/ARXIV.2003.02818},
 url = {https://arxiv.org/abs/2003.02818},
 author = {Swenson, Brian and Murray, Ryan and Poor, H. Vincent and Kar, Soummya},
 keywords = {Optimization and Control (math.OC), FOS: Mathematics, FOS: Mathematics},
 title = {Distributed Stochastic Gradient Descent: Nonconvexity, Nonsmoothness, and Convergence to Local Minima},
 publisher = {arXiv},
 year = {2020},
 copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{liu10,
author = {Liu, Chao and Yang, Hung-chih and Fan, Jinliang and He, Li-Wei and Wang, Yi-Min},
title = {Distributed Nonnegative Matrix Factorization for Web-Scale Dyadic Data Analysis on Mapreduce},
year = {2010},
isbn = {9781605587998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1772690.1772760},
doi = {10.1145/1772690.1772760},
abstract = {The Web abounds with dyadic data that keeps increasing by every single second. Previous work has repeatedly shown the usefulness of extracting the interaction structure inside dyadic data [21, 9, 8]. A commonly used tool in extracting the underlying structure is the matrix factorization, whose fame was further boosted in the Netflix challenge [26]. When we were trying to replicate the same success on real-world Web dyadic data, we were seriously challenged by the scalability of available tools. We therefore in this paper report our efforts on scaling up the nonnegative matrix factorization (NMF) technique. We show that by carefully partitioning the data and arranging the computations to maximize data locality and parallelism, factorizing a tens of millions by hundreds of millions matrix with billions of nonzero cells can be accomplished within tens of hours. This result effectively assures practitioners of the scalability of NMF on Web-scale dyadic data.},
booktitle = {Proceedings of the 19th International Conference on World Wide Web},
pages = {681–690},
numpages = {10},
keywords = {distributed computing, dyadic data, nonnegative matrix factorization, mapreduce},
location = {Raleigh, North Carolina, USA},
series = {WWW '10}
}

@inproceedings{das07,
author = {Das, Abhinandan S. and Datar, Mayur and Garg, Ashutosh and Rajaram, Shyam},
title = {Google News Personalization: Scalable Online Collaborative Filtering},
year = {2007},
isbn = {9781595936547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1242572.1242610},
doi = {10.1145/1242572.1242610},
abstract = {Several approaches to collaborative filtering have been studied but seldom have studies been reported for large (several millionusers and items) and dynamic (the underlying item set is continually changing) settings. In this paper we describe our approach to collaborative filtering for generating personalized recommendations for users of Google News. We generate recommendations using three approaches: collaborative filtering using MinHash clustering, Probabilistic Latent Semantic Indexing (PLSI), and covisitation counts. We combine recommendations from different algorithms using a linear model. Our approach is content agnostic and consequently domain independent, making it easily adaptable for other applications and languages with minimal effort. This paper will describe our algorithms and system setup in detail, and report results of running the recommendations engine on Google News.},
booktitle = {Proceedings of the 16th International Conference on World Wide Web},
pages = {271–280},
numpages = {10},
keywords = {Google news, oneline recommendation system, scalable collaborative filtering, PLSI, personalization, mapreduce, minhash},
location = {Banff, Alberta, Canada},
series = {WWW '07}
}

@article{ZahariaXinEtAl16cacm,
  abstract = {This open source computing framework unifies streaming, batch, and interactive big data workloads to unlock new applications. A simple programming model can capture streaming, batch, and interactive workloads and enable new applications that combine them. Apache Spark applications range from finance to scientific data processing and combine libraries for SQL, machine learning, and graphs. In six years, Apache Spark has grown to 1,000 contributors and thousands of deployments.},
  added-at = {2016-11-02T10:36:14.000+0100},
  author = {Zaharia, Matei and Xin, Reynold S. and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J. and Ghodsi, Ali and Gonzalez, Joseph and Shenker, Scott and Stoica, Ion},
  biburl = {https://www.bibsonomy.org/bibtex/2c215622ad306325525be6cbca36e57e1/flint63},
  doi = {10.1145/2934664},
  file = {ACM Digital Library:2016/ZahariaXinEtAl16cacm.pdf:PDF},
  groups = {public},
  interhash = {b738cb49f24ffce7c52974ab49a09b7f},
  intrahash = {c215622ad306325525be6cbca36e57e1},
  issn = {0001-0782},
  journal = {Communications of the ACM},
  keywords = {01841 acm paper ai data pattern recognition analysis information retrieval tool zzz.big},
  month = {#nov#},
  number = 11,
  pages = {56--65},
  timestamp = {2018-04-16T12:35:05.000+0200},
  title = {{Apache Spark}: A Unified Engine for Big Data Processing},
  username = {flint63},
  volume = 59,
  year = 2016
}

@INPROCEEDINGS{Bennett07thenetflix,
    author = {James Bennett and Stan Lanning and Netflix Netflix},
    title = {The Netflix Prize},
    booktitle = {In KDD Cup and Workshop in conjunction with KDD},
    year = {2007}
}
